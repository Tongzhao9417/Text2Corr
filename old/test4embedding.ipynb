{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "from openai import OpenAI\n",
    "from preprocess import option_prompt_generate,preprocess_data#,behavioral_embedding \n",
    "#preprocess中暂时废弃client，text embedding，behavioral_embedding_model\n",
    "# from Data_loader import create_data_loaders\n",
    "# from model import TextDecisionModel\n",
    "import matplotlib.pyplot as plt\n",
    "import concurrent.futures\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "power = range(2,10,1)\n",
    "gain_S = range(1,20,3)\n",
    "gain_O = range(1,20,3)\n",
    "cost = range(1,20,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_ranges = {'power': power,\n",
    "              'gain_S':gain_S,\n",
    "              'gain_O':gain_O,\n",
    "              'cost':cost}\n",
    "combinations = list(product(*var_ranges.values()))\n",
    "df = pd.DataFrame(combinations, columns=var_ranges.keys())\n",
    "df['total'] = df['gain_S'] + df['gain_O']\n",
    "df['isCheat'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(combinations, columns=var_ranges.keys())\n",
    "df2['total'] = df2['gain_S'] + df2['gain_O']\n",
    "df2['isCheat'] = 0\n",
    "df = pd.concat([df,df2],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prompt.txt',\"r\") as f:\n",
    "    data = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_prompt(total,power,gain_S,gain_O,cost,isCheat,data):\n",
    "    if (isCheat == str(1)):\n",
    "        isCheat_content = '被抽查的玩家猜错并谎报其猜中幸运点数。'\n",
    "    elif (isCheat == str(0)):\n",
    "        isCheat_content = '被抽查的玩家猜中幸运点数。'\n",
    "    replace_dict = {\n",
    "        \"total\":total,\n",
    "        \"power\":power,\n",
    "        \"gain_S\":gain_S,\n",
    "        \"gain_O\":gain_O,\n",
    "        \"cost\":cost,\n",
    "        \"isCheat\":isCheat_content\n",
    "    }\n",
    "    pattern = re.compile(\"|\".join(replace_dict.keys()))\n",
    "    text = pattern.sub(lambda m: replace_dict[m.group(0)], data)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = []\n",
    "\n",
    "for index,row in df.iterrows():\n",
    "    tmp_prompt = replace_prompt(str(row['total']),str(row['power']),str(row['gain_S']),str(row['gain_O']),str(row['cost']),str(row['isCheat']),data)\n",
    "    prompt_list.append(tmp_prompt)\n",
    "# replace_prompt(\"1\",\"2\",\"3\",\"4\",\"5\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每轮有若干名玩家进行猜点数游戏。对每名玩家，计算机程序会随机投出一次骰子，投出的点数称为幸运点数（1-6中的任意数字）。玩家需要猜测本轮幸运的点数并报告。猜中幸运点数有机会获得金币奖励。\n",
      "\n",
      "你的角色是裁判，需要从若干名玩家选择1人，审核TA的报告。\n",
      "\n",
      "如果该玩家报告其未猜中幸运点数，本轮结束，其不会获得相应的金币奖励；\n",
      "如果该玩家报告其猜中幸运点数，你需要对其报告结果进行审核；\n",
      "\n",
      "如果审核通过，对方将获得金钱奖励；\n",
      "如果审核不通过，对方将无法获得金钱奖励\n",
      "\n",
      "本轮奖励总金额为26金币。\n",
      "你从5名玩家中抽查一人。被抽查的玩家猜中幸运点数。\n",
      "\n",
      "TA向你分配13金币\n",
      "\n",
      "如果你接受，你会获得13金币，TA获得剩下13金币，但会有一位第三方损失10金币；\n",
      "如果你拒绝，你和TA会获得0金币，第三方会损失0金币 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_list[4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url='https://api.ohmygpt.com/v1',\n",
    "    api_key='sk-byGN2JFH6F68b7814673T3BLbkFJB117792eDB154451aBbf') #add you own API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text):\n",
    "    response = client.embeddings.create(\n",
    "            model='text-embedding-3-large',\n",
    "            input=text,\n",
    "            dimensions=1536)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_list = []  \n",
    "# for i in range(c13k_problems.shape[0]): #逐条处理\n",
    "#     tmp_data = c13k_problems.iloc[i,:]\n",
    "#     tmp_prompt = option_prompt_generate(tmp_data['p'],tmp_data['v'])\n",
    "#     tmp_prompt = np.array(tmp_prompt, dtype ='object')\n",
    "#     prompt_list.append(tmp_prompt)\n",
    "\n",
    "# 循环结束后，一次性将所有prompt合并为numpy数组\n",
    "prompt_dataset = np.vstack(prompt_list)\n",
    "\n",
    "# 如果需要保持二维结构且列数为1，可以添加以下代码：\n",
    "if len(prompt_dataset.shape) == 1:\n",
    "    prompt_dataset = prompt_dataset.reshape(-1, 1)\n",
    "\n",
    "# 优化后时间缩短：7.9s to 1.0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_worker = 20\n",
    "embedding_result = []\n",
    "# pbar = tqdm(total = len(prompt_dataset))\n",
    "# with tqdm(total = len(prompt_dataset)) as pbar:\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_worker) as executor:\n",
    "    futures = [executor.submit(get_embeddings, texts) for texts in prompt_dataset]\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures),total=len(prompt_dataset)):\n",
    "        embedding_result.append(future.result().data[0].embedding)\n",
    "            # pbar.update()\n",
    "\n",
    "# pbar.close()\n",
    "# print(embedding_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_problem_embeddings = np.array(embedding_result, dtype=np.float32)\n",
    "np.save(\"./result/text_problem_embeddings_0315.npy\", text_problem_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_problem_embeddings = np.load(\"../result/text_problem_embeddings_0315.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_embedding = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "##2. construct dataloader, model for training\n",
    "class JointDataset(Dataset):\n",
    "    def __init__(self, text_embeddings, decision_embeddings, labels):\n",
    "        self.text_embeddings = text_embeddings\n",
    "        self.decision_embeddings = decision_embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "        # Ensure both embeddings have the same number of samples\n",
    "        assert self._get_num_samples(self.text_embeddings) == self._get_num_samples(self.decision_embeddings), \"Mismatched number of samples between text and decision embeddings.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._get_num_samples(self.text_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"text_embedding\": self.text_embeddings[idx],\n",
    "            \"decision_embedding\": self.decision_embeddings[idx],\n",
    "            \"label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "    def _get_num_samples(self, data):\n",
    "        if isinstance(data, (list, tuple)):\n",
    "            return len(data)\n",
    "        elif isinstance(data, (np.ndarray, torch.Tensor)):\n",
    "            return data.shape[0]\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported type for embeddings\")\n",
    "            \n",
    "def generate_labels(embeddings_1, embeddings_2, false_scale=1):\n",
    "    # True labels\n",
    "    true_labels = np.ones(embeddings_1.shape[0])\n",
    "    \n",
    "    # Initialize lists to hold the combined embeddings and labels\n",
    "    combined_embeddings_1 = list(embeddings_1)\n",
    "    combined_embeddings_2 = list(embeddings_2)\n",
    "    combined_labels = list(true_labels)\n",
    "    \n",
    "    for _ in range(false_scale):\n",
    "        # Shuffling embeddings_2 for false labels\n",
    "        shuffled_embeddings_2 = np.random.permutation(embeddings_2)\n",
    "        \n",
    "        # Append false data and labels\n",
    "        combined_embeddings_1.extend(embeddings_1)\n",
    "        combined_embeddings_2.extend(shuffled_embeddings_2)\n",
    "        combined_labels.extend(np.zeros(embeddings_1.shape[0]))  # false_labels\n",
    "        \n",
    "    # Convert lists back to numpy arrays\n",
    "    combined_embeddings_1 = np.array(combined_embeddings_1)\n",
    "    combined_embeddings_2 = np.array(combined_embeddings_2)\n",
    "    combined_labels = np.array(combined_labels)\n",
    "    \n",
    "    return combined_embeddings_1, combined_embeddings_2, combined_labels  \n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, text_embeddings, decision_embeddings):\n",
    "        self.text_embeddings = text_embeddings\n",
    "        self.decision_embeddings = decision_embeddings\n",
    "\n",
    "        # Ensure both embeddings have the same number of samples\n",
    "        assert self._get_num_samples(self.text_embeddings) == self._get_num_samples(self.decision_embeddings), \"Mismatched number of samples between text and decision embeddings.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._get_num_samples(self.text_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"text_embedding\": self.text_embeddings[idx],\n",
    "            \"decision_embedding\": self.decision_embeddings[idx],\n",
    "        }\n",
    "\n",
    "    def _get_num_samples(self, data):\n",
    "        if isinstance(data, (list, tuple)):\n",
    "            return len(data)\n",
    "        elif isinstance(data, (np.ndarray, torch.Tensor)):\n",
    "            return data.shape[0]\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported type for embeddings\")\n",
    "            \n",
    "         \n",
    "def create_data_loaders(text,decision, batch_size, test_size, contrastive = True, false_scale=1, scale = 'min_max'):\n",
    "    ##generate true matching and mismatching labels\n",
    "    if scale == 'min_max':\n",
    "    # Initialize the scaler\n",
    "        scaler = MinMaxScaler()\n",
    "        \n",
    "        # Fit and transform the data\n",
    "        decision = scaler.fit_transform(decision)\n",
    "    elif scale == 'outcome_scaling':\n",
    "        decision[:,[0,1,2,3,4,5]] =  decision[:,[0,1,2,3,4,5]]/1000\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "        \n",
    "    if contrastive:\n",
    "        behavioral_embedding_dataset, text_problem_embeddings, label_dataset= generate_labels(decision, text, false_scale)\n",
    "        label_dataset = np.array(label_dataset, dtype = 'float32')\n",
    "    \n",
    "        # Split into training and temporary sets (80% training, 20% temp)\n",
    "        behavioral_embedding_train, behavioral_embedding_temp, text_problem_train, text_problem_temp, labels_train, labels_temp = train_test_split(behavioral_embedding_dataset, text_problem_embeddings, label_dataset, test_size=test_size, stratify=label_dataset)\n",
    "    \n",
    "        # Split the temporary set into validation and test sets (50% validation, 50% test of the remaining 20%)\n",
    "        behavioral_embedding_val, behavioral_embedding_test, text_problem_val, text_problem_test, labels_val, labels_test = train_test_split(behavioral_embedding_temp, text_problem_temp, labels_temp, test_size=0.5, stratify=labels_temp)\n",
    "    \n",
    "        # First, we'll create Datasets for each split\n",
    "        train_dataset = JointDataset(text_problem_train, behavioral_embedding_train, labels_train)\n",
    "        val_dataset = JointDataset(text_problem_val, behavioral_embedding_val, labels_val)\n",
    "        test_dataset = JointDataset(text_problem_test, behavioral_embedding_test, labels_test)\n",
    "    else:\n",
    "        \n",
    "        # Split into training and temporary sets (80% training, 20% temp)\n",
    "        behavioral_embedding_train, behavioral_embedding_temp, text_problem_train, text_problem_temp = train_test_split(decision, text, test_size=test_size)\n",
    "    \n",
    "        # Split the temporary set into validation and test sets (50% validation, 50% test of the remaining 20%)\n",
    "        behavioral_embedding_val, behavioral_embedding_test, text_problem_val, text_problem_test = train_test_split(behavioral_embedding_temp, text_problem_temp, test_size=0.5)\n",
    "    \n",
    "        # First, we'll create Datasets for each split\n",
    "        train_dataset = Dataset(text_problem_train, behavioral_embedding_train)\n",
    "        val_dataset = Dataset(text_problem_val, behavioral_embedding_val)\n",
    "        test_dataset = Dataset(text_problem_test, behavioral_embedding_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 256\n",
    "train_dataloader, val_dataloader,test_dataloader = create_data_loaders(text_problem_embeddings, behavior_embedding, batch_size=batch_size, test_size = 0.2, contrastive = True, false_scale = 2, scale = 'min_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(behavior_embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=0.9):\n",
    "        super(CosineContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # Cosine similarity\n",
    "        cosine_sim = nn.functional.cosine_similarity(output1, output2)\n",
    "        \n",
    "        # For similar pairs: we want the negative of (1 - cosine_sim) to make it closer to 1\n",
    "        # For dissimilar pairs: we take max(0, cosine_sim - margin) to push them apart until a certain margin\n",
    "        loss_contrastive = torch.mean(torch.mean((label) * torch.pow(1 - cosine_sim, 2)) +\n",
    "                                      torch.mean((1-label) * torch.pow(torch.clamp(cosine_sim - self.margin, min=0.0), 2)))\n",
    "        return loss_contrastive\n",
    "    \n",
    "class FeatureLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureLoss, self).__init__()\n",
    "        \n",
    "    \n",
    "    def forward(self, output1, output2):\n",
    "        # Standardize each dimension\n",
    "        # std1 = torch.std(output1, dim=0, keepdim=True) + 1e-8  # Avoid division by zero\n",
    "        # mean1 = torch.mean(output1, dim=0, keepdim=True)\n",
    "        # output1 = (output1 - mean1) / std1\n",
    "        \n",
    "        # std2 = torch.std(output2, dim=0, keepdim=True) + 1e-8  # Avoid division by zero\n",
    "        # mean2 = torch.mean(output2, dim=0, keepdim=True)\n",
    "        # output2 = (output2 - mean2) / std2\n",
    "        \n",
    "        feature_loss = torch.square(output1 - output2)\n",
    "        return feature_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TextDecisionModel(nn.Module):\n",
    "    def __init__(self, text_dim=1536, decision_dim=6):\n",
    "        super(TextDecisionModel, self).__init__()\n",
    "        \n",
    "        self.text_proj = nn.Sequential(\n",
    "            nn.Linear(text_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, decision_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, text):\n",
    "        return self.text_proj(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = JointEmbedding().to(device)\n",
    "model = TextDecisionModel().to(device)\n",
    "# Define the loss, optimizer, etc.\n",
    "criterion = FeatureLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 200\n",
    "cosine_similarity_threshold = 0.9 # This can be adjusted based on your needs\n",
    "\n",
    "feature_losses = {f\"dim_{i}\": [] for i in range(6)}\n",
    "val_feature_losses = {f\"dim_{i}\": [] for i in range(6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    # true_pairs_similarity = []\n",
    "    # false_pairs_similarity = []\n",
    "    # For storing dimensional losses in an epoch\n",
    "    epoch_dimensional_losses = {f\"dim_{i}\": [] for i in range(6)}\n",
    "    val_epoch_dimensional_losses = {f\"dim_{i}\": [] for i in range(6)}\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        text_embeds = batch[\"text_embedding\"].to(device)\n",
    "        decision_embeds = batch[\"decision_embedding\"].to(device)\n",
    "        # labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # text_outputs, behavioral_outputs = model(text_embeds, decision_embeds)\n",
    "        \n",
    "        decision_pred = model(text_embeds)\n",
    "        #dimensional-wise loss to track the training\n",
    "        feature_loss = criterion(decision_pred, decision_embeds)\n",
    "        #MSE loss indeed\n",
    "        loss = torch.mean(torch.sum(feature_loss, dim=1))\n",
    "        # loss = criterion(text_outputs, behavioral_outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        for i in range(6):\n",
    "            epoch_dimensional_losses[f\"dim_{i}\"].append(feature_loss[:, i].mean().item())\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Store the average dimensional loss for this epoch\n",
    "    for i in range(6):\n",
    "        feature_losses[f\"dim_{i}\"].append(np.mean(epoch_dimensional_losses[f\"dim_{i}\"]))\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {total_loss/len(train_dataloader)}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct_pairs = 0\n",
    "        total_pairs = 0\n",
    "        total_val_loss = 0\n",
    "        # true_pairs_val_similarity = []\n",
    "        # false_pairs_val_similarity = []\n",
    "\n",
    "        for batch in test_dataloader:\n",
    "            text_emb = batch[\"text_embedding\"].to(device)\n",
    "            decision_emb = batch[\"decision_embedding\"].to(device)\n",
    "            label = batch[\"label\"].to(device)\n",
    "    \n",
    "            text_proj = model(text_emb)\n",
    "            cosine_sim = nn.functional.cosine_similarity(text_proj, decision_emb)\n",
    "            # loss = criterion(text_proj, decision_proj, label)\n",
    "            decision_pred = model(text_embeds)\n",
    "            val_feature_loss = criterion(decision_pred, decision_embeds)\n",
    "            loss = torch.mean(torch.sum(val_feature_loss, dim=1))\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            # # Calculate similarity for validation\n",
    "            # true_pairs_val_similarity.extend(cosine_sim[label == 1].cpu().numpy())\n",
    "            # false_pairs_val_similarity.extend(cosine_sim[label == 0].cpu().numpy())\n",
    "\n",
    "            # # Check pairs based on cosine similarity\n",
    "            correct_pairs += ((cosine_sim > cosine_similarity_threshold) & (label == 1)).sum().item()\n",
    "            correct_pairs += ((cosine_sim <= cosine_similarity_threshold) & (label == 0)).sum().item()\n",
    "            total_pairs += label.size(0)\n",
    "            \n",
    "            for i in range(6):\n",
    "                val_epoch_dimensional_losses[f\"dim_{i}\"].append(val_feature_loss[:, i].mean().item())\n",
    "        \n",
    "        # Store the average dimensional loss for this epoch\n",
    "        for i in range(6):\n",
    "            val_feature_losses[f\"dim_{i}\"].append(np.mean(val_epoch_dimensional_losses[f\"dim_{i}\"]))\n",
    "                \n",
    "        # true_avg_val_sim = np.mean(true_pairs_val_similarity)\n",
    "        # false_avg_val_sim = np.mean(false_pairs_val_similarity)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} - Validation Loss: {total_val_loss/len(test_dataloader)}\")\n",
    "        # print(f\"Average True Matching Similarity (Validation): {true_avg_val_sim:.2f}\")\n",
    "        # print(f\"Average False Matching Similarity (Validation): {false_avg_val_sim:.2f}\")\n",
    "        # print(f\"Difference (Validation): {true_avg_val_sim - false_avg_val_sim:.2f}\\n\")\n",
    "        print(f\"Validation Accuracy: {correct_pairs / total_pairs * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model.__class__.__name__ +\"_\"+f\"{datetime.now().month:02d}\"+f\"{datetime.now().day:02d}\"+\"_\"+ '.pth'\n",
    "torch.save(model.state_dict(), 'result/' + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "for i in range(6):\n",
    "    plt.plot(feature_losses[f\"dim_{i}\"], label=f\"Dimension {i}\")\n",
    "plt.legend()\n",
    "plt.title(\"Dimension-wise Training Losses over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "# Save the plot with a specific DPI\n",
    "plt.savefig('pic/training_loss_0315.png', dpi=300)  # 300 DPI is a common high-resolution setting\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "for i in range(6):\n",
    "    plt.plot(val_feature_losses[f\"dim_{i}\"], label=f\"Dimension {i}\")\n",
    "plt.legend()\n",
    "plt.title(\"Dimension-wise Validation Losses over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "# Save the plot with a specific DPI\n",
    "plt.savefig('pic/val_loss.png', dpi=300)  # 300 DPI is a common high-resolution setting\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
