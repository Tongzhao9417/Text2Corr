{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "from openai import OpenAI\n",
    "from preprocess import option_prompt_generate,preprocess_data#,behavioral_embedding \n",
    "#preprocess中暂时废弃client，text embedding，behavioral_embedding_model\n",
    "# from Data_loader import create_data_loaders\n",
    "# from model import TextDecisionModel\n",
    "import matplotlib.pyplot as plt\n",
    "import concurrent.futures\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power = range(2,10,1)\n",
    "gain_S = range(1,20,3)\n",
    "gain_O = range(1,20,3)\n",
    "cost = range(1,20,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_ranges = {'power': power,\n",
    "              'gain_S':gain_S,\n",
    "              'gain_O':gain_O,\n",
    "              'cost':cost}\n",
    "combinations = list(product(*var_ranges.values()))\n",
    "df = pd.DataFrame(combinations, columns=var_ranges.keys())\n",
    "df['total'] = df['gain_S'] + df['gain_O']\n",
    "df['isCheat'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(combinations, columns=var_ranges.keys())\n",
    "df2['total'] = df2['gain_S'] + df2['gain_O']\n",
    "df2['isCheat'] = 0\n",
    "df = pd.concat([df,df2],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prompt.txt',\"r\") as f:\n",
    "    data = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_prompt(total,power,gain_S,gain_O,cost,isCheat,data):\n",
    "    if (isCheat == str(1)):\n",
    "        isCheat_content = '被抽查的玩家猜错并谎报其猜中幸运点数。'\n",
    "    elif (isCheat == str(0)):\n",
    "        isCheat_content = '被抽查的玩家猜中幸运点数。'\n",
    "    replace_dict = {\n",
    "        \"total\":total,\n",
    "        \"power\":power,\n",
    "        \"gain_S\":gain_S,\n",
    "        \"gain_O\":gain_O,\n",
    "        \"cost\":cost,\n",
    "        \"isCheat\":isCheat_content\n",
    "    }\n",
    "    pattern = re.compile(\"|\".join(replace_dict.keys()))\n",
    "    text = pattern.sub(lambda m: replace_dict[m.group(0)], data)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = []\n",
    "\n",
    "for index,row in df.iterrows():\n",
    "    tmp_prompt = replace_prompt(str(row['total']),str(row['power']),str(row['gain_S']),str(row['gain_O']),str(row['cost']),str(row['isCheat']),data)\n",
    "    prompt_list.append(tmp_prompt)\n",
    "# replace_prompt(\"1\",\"2\",\"3\",\"4\",\"5\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_list[4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url='https://api.ohmygpt.com/v1',\n",
    "    api_key='sk-byGN2JFH6F68b7814673T3BLbkFJB117792eDB154451aBbf') #add you own API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text):\n",
    "    response = client.embeddings.create(\n",
    "            model='text-embedding-3-large',\n",
    "            input=text,\n",
    "            dimensions=1536)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_list = []  \n",
    "# for i in range(c13k_problems.shape[0]): #逐条处理\n",
    "#     tmp_data = c13k_problems.iloc[i,:]\n",
    "#     tmp_prompt = option_prompt_generate(tmp_data['p'],tmp_data['v'])\n",
    "#     tmp_prompt = np.array(tmp_prompt, dtype ='object')\n",
    "#     prompt_list.append(tmp_prompt)\n",
    "\n",
    "# 循环结束后，一次性将所有prompt合并为numpy数组\n",
    "prompt_dataset = np.vstack(prompt_list)\n",
    "\n",
    "# 如果需要保持二维结构且列数为1，可以添加以下代码：\n",
    "if len(prompt_dataset.shape) == 1:\n",
    "    prompt_dataset = prompt_dataset.reshape(-1, 1)\n",
    "\n",
    "# 优化后时间缩短：7.9s to 1.0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_worker = 20\n",
    "embedding_result = []\n",
    "# pbar = tqdm(total = len(prompt_dataset))\n",
    "# with tqdm(total = len(prompt_dataset)) as pbar:\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_worker) as executor:\n",
    "    futures = [executor.submit(get_embeddings, texts) for texts in prompt_dataset]\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures),total=len(prompt_dataset)):\n",
    "        embedding_result.append(future.result().data[0].embedding)\n",
    "            # pbar.update()\n",
    "\n",
    "# pbar.close()\n",
    "# print(embedding_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_problem_embeddings = np.array(embedding_result, dtype=np.float32)\n",
    "np.save(\"./result/text_problem_embeddings_0315.npy\", text_problem_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_problem_embeddings = np.load(\"./result/text_problem_embeddings_0315.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_embedding = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "##2. construct dataloader, model for training\n",
    "class JointDataset(Dataset):\n",
    "    def __init__(self, text_embeddings, decision_embeddings, labels):\n",
    "        self.text_embeddings = text_embeddings\n",
    "        self.decision_embeddings = decision_embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "        # Ensure both embeddings have the same number of samples\n",
    "        assert self._get_num_samples(self.text_embeddings) == self._get_num_samples(self.decision_embeddings), \"Mismatched number of samples between text and decision embeddings.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._get_num_samples(self.text_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"text_embedding\": self.text_embeddings[idx],\n",
    "            \"decision_embedding\": self.decision_embeddings[idx],\n",
    "            \"label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "    def _get_num_samples(self, data):\n",
    "        if isinstance(data, (list, tuple)):\n",
    "            return len(data)\n",
    "        elif isinstance(data, (np.ndarray, torch.Tensor)):\n",
    "            return data.shape[0]\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported type for embeddings\")\n",
    "            \n",
    "def generate_labels(embeddings_1, embeddings_2, false_scale=1):\n",
    "    # True labels\n",
    "    true_labels = np.ones(embeddings_1.shape[0])\n",
    "    \n",
    "    # Initialize lists to hold the combined embeddings and labels\n",
    "    combined_embeddings_1 = list(embeddings_1)\n",
    "    combined_embeddings_2 = list(embeddings_2)\n",
    "    combined_labels = list(true_labels)\n",
    "    \n",
    "    for _ in range(false_scale):\n",
    "        # Shuffling embeddings_2 for false labels\n",
    "        shuffled_embeddings_2 = np.random.permutation(embeddings_2)\n",
    "        \n",
    "        # Append false data and labels\n",
    "        combined_embeddings_1.extend(embeddings_1)\n",
    "        combined_embeddings_2.extend(shuffled_embeddings_2)\n",
    "        combined_labels.extend(np.zeros(embeddings_1.shape[0]))  # false_labels\n",
    "        \n",
    "    # Convert lists back to numpy arrays\n",
    "    combined_embeddings_1 = np.array(combined_embeddings_1)\n",
    "    combined_embeddings_2 = np.array(combined_embeddings_2)\n",
    "    combined_labels = np.array(combined_labels)\n",
    "    \n",
    "    return combined_embeddings_1, combined_embeddings_2, combined_labels  \n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, text_embeddings, decision_embeddings):\n",
    "        self.text_embeddings = text_embeddings\n",
    "        self.decision_embeddings = decision_embeddings\n",
    "\n",
    "        # Ensure both embeddings have the same number of samples\n",
    "        assert self._get_num_samples(self.text_embeddings) == self._get_num_samples(self.decision_embeddings), \"Mismatched number of samples between text and decision embeddings.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._get_num_samples(self.text_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"text_embedding\": self.text_embeddings[idx],\n",
    "            \"decision_embedding\": self.decision_embeddings[idx],\n",
    "        }\n",
    "\n",
    "    def _get_num_samples(self, data):\n",
    "        if isinstance(data, (list, tuple)):\n",
    "            return len(data)\n",
    "        elif isinstance(data, (np.ndarray, torch.Tensor)):\n",
    "            return data.shape[0]\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported type for embeddings\")\n",
    "            \n",
    "         \n",
    "def create_data_loaders(text,decision, batch_size, test_size, contrastive = True, false_scale=1, scale = 'min_max'):\n",
    "    ##generate true matching and mismatching labels\n",
    "    if scale == 'min_max':\n",
    "    # Initialize the scaler\n",
    "        scaler = MinMaxScaler()\n",
    "        \n",
    "        # Fit and transform the data\n",
    "        decision = scaler.fit_transform(decision)\n",
    "    elif scale == 'outcome_scaling':\n",
    "        decision[:,[0,1,2,3,4,5]] =  decision[:,[0,1,2,3,4,5]]/1000\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "        \n",
    "    if contrastive:\n",
    "        behavioral_embedding_dataset, text_problem_embeddings, label_dataset= generate_labels(decision, text, false_scale)\n",
    "        label_dataset = np.array(label_dataset, dtype = 'float32')\n",
    "    \n",
    "        # Split into training and temporary sets (80% training, 20% temp)\n",
    "        behavioral_embedding_train, behavioral_embedding_temp, text_problem_train, text_problem_temp, labels_train, labels_temp = train_test_split(behavioral_embedding_dataset, text_problem_embeddings, label_dataset, test_size=test_size, stratify=label_dataset)\n",
    "    \n",
    "        # Split the temporary set into validation and test sets (50% validation, 50% test of the remaining 20%)\n",
    "        behavioral_embedding_val, behavioral_embedding_test, text_problem_val, text_problem_test, labels_val, labels_test = train_test_split(behavioral_embedding_temp, text_problem_temp, labels_temp, test_size=0.5, stratify=labels_temp)\n",
    "    \n",
    "        # First, we'll create Datasets for each split\n",
    "        train_dataset = JointDataset(text_problem_train, behavioral_embedding_train, labels_train)\n",
    "        val_dataset = JointDataset(text_problem_val, behavioral_embedding_val, labels_val)\n",
    "        test_dataset = JointDataset(text_problem_test, behavioral_embedding_test, labels_test)\n",
    "    else:\n",
    "        \n",
    "        # Split into training and temporary sets (80% training, 20% temp)\n",
    "        behavioral_embedding_train, behavioral_embedding_temp, text_problem_train, text_problem_temp = train_test_split(decision, text, test_size=test_size)\n",
    "    \n",
    "        # Split the temporary set into validation and test sets (50% validation, 50% test of the remaining 20%)\n",
    "        behavioral_embedding_val, behavioral_embedding_test, text_problem_val, text_problem_test = train_test_split(behavioral_embedding_temp, text_problem_temp, test_size=0.5)\n",
    "    \n",
    "        # First, we'll create Datasets for each split\n",
    "        train_dataset = Dataset(text_problem_train, behavioral_embedding_train)\n",
    "        val_dataset = Dataset(text_problem_val, behavioral_embedding_val)\n",
    "        test_dataset = Dataset(text_problem_test, behavioral_embedding_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 256\n",
    "train_dataloader, val_dataloader,test_dataloader = create_data_loaders(text_problem_embeddings, behavior_embedding, batch_size=batch_size, test_size = 0.2, contrastive = True, false_scale = 2, scale = 'min_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(behavior_embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=0.9):\n",
    "        super(CosineContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # Cosine similarity\n",
    "        cosine_sim = nn.functional.cosine_similarity(output1, output2)\n",
    "        \n",
    "        # For similar pairs: we want the negative of (1 - cosine_sim) to make it closer to 1\n",
    "        # For dissimilar pairs: we take max(0, cosine_sim - margin) to push them apart until a certain margin\n",
    "        loss_contrastive = torch.mean(torch.mean((label) * torch.pow(1 - cosine_sim, 2)) +\n",
    "                                      torch.mean((1-label) * torch.pow(torch.clamp(cosine_sim - self.margin, min=0.0), 2)))\n",
    "        return loss_contrastive\n",
    "    \n",
    "class FeatureLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureLoss, self).__init__()\n",
    "        \n",
    "    \n",
    "    def forward(self, output1, output2):\n",
    "        # Standardize each dimension\n",
    "        # std1 = torch.std(output1, dim=0, keepdim=True) + 1e-8  # Avoid division by zero\n",
    "        # mean1 = torch.mean(output1, dim=0, keepdim=True)\n",
    "        # output1 = (output1 - mean1) / std1\n",
    "        \n",
    "        # std2 = torch.std(output2, dim=0, keepdim=True) + 1e-8  # Avoid division by zero\n",
    "        # mean2 = torch.mean(output2, dim=0, keepdim=True)\n",
    "        # output2 = (output2 - mean2) / std2\n",
    "        \n",
    "        feature_loss = torch.square(output1 - output2)\n",
    "        return feature_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TextDecisionModel(nn.Module):\n",
    "    def __init__(self, text_dim=1536, decision_dim=6):\n",
    "        super(TextDecisionModel, self).__init__()\n",
    "        \n",
    "        self.text_proj = nn.Sequential(\n",
    "            nn.Linear(text_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, decision_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, text):\n",
    "        return self.text_proj(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = JointEmbedding().to(device)\n",
    "model = TextDecisionModel().to(device)\n",
    "# Define the loss, optimizer, etc.\n",
    "criterion = FeatureLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 200\n",
    "cosine_similarity_threshold = 0.9 # This can be adjusted based on your needs\n",
    "\n",
    "feature_losses = {f\"dim_{i}\": [] for i in range(6)}\n",
    "val_feature_losses = {f\"dim_{i}\": [] for i in range(6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    # true_pairs_similarity = []\n",
    "    # false_pairs_similarity = []\n",
    "    # For storing dimensional losses in an epoch\n",
    "    epoch_dimensional_losses = {f\"dim_{i}\": [] for i in range(6)}\n",
    "    val_epoch_dimensional_losses = {f\"dim_{i}\": [] for i in range(6)}\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        text_embeds = batch[\"text_embedding\"].to(device)\n",
    "        decision_embeds = batch[\"decision_embedding\"].to(device)\n",
    "        # labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # text_outputs, behavioral_outputs = model(text_embeds, decision_embeds)\n",
    "        \n",
    "        decision_pred = model(text_embeds)\n",
    "        #dimensional-wise loss to track the training\n",
    "        feature_loss = criterion(decision_pred, decision_embeds)\n",
    "        #MSE loss indeed\n",
    "        loss = torch.mean(torch.sum(feature_loss, dim=1))\n",
    "        # loss = criterion(text_outputs, behavioral_outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        for i in range(6):\n",
    "            epoch_dimensional_losses[f\"dim_{i}\"].append(feature_loss[:, i].mean().item())\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Store the average dimensional loss for this epoch\n",
    "    for i in range(6):\n",
    "        feature_losses[f\"dim_{i}\"].append(np.mean(epoch_dimensional_losses[f\"dim_{i}\"]))\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {total_loss/len(train_dataloader)}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct_pairs = 0\n",
    "        total_pairs = 0\n",
    "        total_val_loss = 0\n",
    "        # true_pairs_val_similarity = []\n",
    "        # false_pairs_val_similarity = []\n",
    "\n",
    "        for batch in test_dataloader:\n",
    "            text_emb = batch[\"text_embedding\"].to(device)\n",
    "            decision_emb = batch[\"decision_embedding\"].to(device)\n",
    "            label = batch[\"label\"].to(device)\n",
    "    \n",
    "            text_proj = model(text_emb)\n",
    "            cosine_sim = nn.functional.cosine_similarity(text_proj, decision_emb)\n",
    "            # loss = criterion(text_proj, decision_proj, label)\n",
    "            decision_pred = model(text_embeds)\n",
    "            val_feature_loss = criterion(decision_pred, decision_embeds)\n",
    "            loss = torch.mean(torch.sum(val_feature_loss, dim=1))\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            # # Calculate similarity for validation\n",
    "            # true_pairs_val_similarity.extend(cosine_sim[label == 1].cpu().numpy())\n",
    "            # false_pairs_val_similarity.extend(cosine_sim[label == 0].cpu().numpy())\n",
    "\n",
    "            # # Check pairs based on cosine similarity\n",
    "            correct_pairs += ((cosine_sim > cosine_similarity_threshold) & (label == 1)).sum().item()\n",
    "            correct_pairs += ((cosine_sim <= cosine_similarity_threshold) & (label == 0)).sum().item()\n",
    "            total_pairs += label.size(0)\n",
    "            \n",
    "            for i in range(6):\n",
    "                val_epoch_dimensional_losses[f\"dim_{i}\"].append(val_feature_loss[:, i].mean().item())\n",
    "        \n",
    "        # Store the average dimensional loss for this epoch\n",
    "        for i in range(6):\n",
    "            val_feature_losses[f\"dim_{i}\"].append(np.mean(val_epoch_dimensional_losses[f\"dim_{i}\"]))\n",
    "                \n",
    "        # true_avg_val_sim = np.mean(true_pairs_val_similarity)\n",
    "        # false_avg_val_sim = np.mean(false_pairs_val_similarity)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} - Validation Loss: {total_val_loss/len(test_dataloader)}\")\n",
    "        # print(f\"Average True Matching Similarity (Validation): {true_avg_val_sim:.2f}\")\n",
    "        # print(f\"Average False Matching Similarity (Validation): {false_avg_val_sim:.2f}\")\n",
    "        # print(f\"Difference (Validation): {true_avg_val_sim - false_avg_val_sim:.2f}\\n\")\n",
    "        print(f\"Validation Accuracy: {correct_pairs / total_pairs * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model.__class__.__name__ +\"_\"+f\"{datetime.now().month:02d}\"+f\"{datetime.now().day:02d}\"+\"_\"+ '.pth'\n",
    "torch.save(model.state_dict(), 'result/' + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "for i in range(6):\n",
    "    plt.plot(feature_losses[f\"dim_{i}\"], label=f\"Dimension {i}\")\n",
    "plt.legend()\n",
    "plt.title(\"Dimension-wise Training Losses over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "# Save the plot with a specific DPI\n",
    "plt.savefig('pic/training_loss_0315.png', dpi=300)  # 300 DPI is a common high-resolution setting\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "for i in range(6):\n",
    "    plt.plot(val_feature_losses[f\"dim_{i}\"], label=f\"Dimension {i}\")\n",
    "plt.legend()\n",
    "plt.title(\"Dimension-wise Validation Losses over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "# Save the plot with a specific DPI\n",
    "plt.savefig('pic/val_loss.png', dpi=300)  # 300 DPI is a common high-resolution setting\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
